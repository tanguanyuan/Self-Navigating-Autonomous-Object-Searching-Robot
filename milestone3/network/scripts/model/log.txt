Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-09 10:12:04 ===============
=> Current Lr: 0.001
[0/125]: 1.5420
[20/125]: 0.1391
[40/125]: 0.1354
[60/125]: 0.0897
[80/125]: 0.0720
[100/125]: 0.0510
[120/125]: 0.0623
=> Training Loss: 0.1179, Evaluation Loss 0.0543

============= Epoch 1 | 2022-09-09 10:13:05 ===============
=> Current Lr: 0.001
[0/125]: 0.0514
[20/125]: 0.0535
[40/125]: 0.0471
[60/125]: 0.0456
[80/125]: 0.0550
[100/125]: 0.0471
[120/125]: 0.0455
=> Training Loss: 0.0478, Evaluation Loss 0.0456

============= Epoch 2 | 2022-09-09 10:13:59 ===============
=> Current Lr: 0.001
[0/125]: 0.0449
[20/125]: 0.0315
[40/125]: 0.0346
[60/125]: 0.0335
[80/125]: 0.0339
[100/125]: 0.0241
[120/125]: 0.0261
=> Training Loss: 0.0348, Evaluation Loss 0.0415

============= Epoch 3 | 2022-09-09 10:14:55 ===============
=> Current Lr: 0.001
[0/125]: 0.0350
[20/125]: 0.0308
[40/125]: 0.0249
[60/125]: 0.0221
[80/125]: 0.0327
[100/125]: 0.0375
[120/125]: 0.0272
=> Training Loss: 0.0303, Evaluation Loss 0.0272

============= Epoch 4 | 2022-09-09 10:15:51 ===============
=> Current Lr: 0.001
[0/125]: 0.0350
[20/125]: 0.0256
[40/125]: 0.0307
[60/125]: 0.0286
[80/125]: 0.0298
[100/125]: 0.0244
[120/125]: 0.0218
=> Training Loss: 0.0257, Evaluation Loss 0.0601

============= Epoch 5 | 2022-09-09 10:16:49 ===============
=> Current Lr: 0.0005
[0/125]: 0.0305
[20/125]: 0.0186
[40/125]: 0.0182
[60/125]: 0.0178
[80/125]: 0.0169
[100/125]: 0.0141
[120/125]: 0.0166
=> Training Loss: 0.0169, Evaluation Loss 0.0159

============= Epoch 6 | 2022-09-09 10:17:47 ===============
=> Current Lr: 0.0005
[0/125]: 0.0167
[20/125]: 0.0139
[40/125]: 0.0174
[60/125]: 0.0215
[80/125]: 0.0154
[100/125]: 0.0143
[120/125]: 0.0166
=> Training Loss: 0.0151, Evaluation Loss 0.0159

============= Epoch 7 | 2022-09-09 10:18:46 ===============
=> Current Lr: 0.0005
[0/125]: 0.0118
[20/125]: 0.0139
[40/125]: 0.0118
[60/125]: 0.0097
[80/125]: 0.0142
[100/125]: 0.0136
[120/125]: 0.0160
=> Training Loss: 0.0134, Evaluation Loss 0.0174

============= Epoch 8 | 2022-09-09 10:19:45 ===============
=> Current Lr: 0.0005
[0/125]: 0.0136
[20/125]: 0.0112
[40/125]: 0.0114
[60/125]: 0.0129
[80/125]: 0.0163
[100/125]: 0.0156
[120/125]: 0.0100
=> Training Loss: 0.0141, Evaluation Loss 0.0202

============= Epoch 9 | 2022-09-09 10:20:45 ===============
=> Current Lr: 0.0005
[0/125]: 0.0165
[20/125]: 0.0140
[40/125]: 0.0130
[60/125]: 0.0155
[80/125]: 0.0101
[100/125]: 0.0100
[120/125]: 0.0104
=> Training Loss: 0.0128, Evaluation Loss 0.0156

============= Epoch 10 | 2022-09-09 10:36:45 ==============
=> Current Lr: 0.00025
[0/125]: 0.0089
[20/125]: 0.0113
[40/125]: 0.0099
[60/125]: 0.0103
[80/125]: 0.0093
[100/125]: 0.0109
[120/125]: 0.0080
=> Training Loss: 0.0098, Evaluation Loss 0.0092

============= Epoch 11 | 2022-09-09 10:37:45 ==============
=> Current Lr: 0.00025
[0/125]: 0.0075
[20/125]: 0.0080
[40/125]: 0.0074
[60/125]: 0.0083
[80/125]: 0.0094
[100/125]: 0.0100
[120/125]: 0.0097
=> Training Loss: 0.0093, Evaluation Loss 0.0093

============= Epoch 12 | 2022-09-09 10:38:47 ==============
=> Current Lr: 0.00025
[0/125]: 0.0123
[20/125]: 0.0088
[40/125]: 0.0085
[60/125]: 0.0088
[80/125]: 0.0103
[100/125]: 0.0161
[120/125]: 0.0113
=> Training Loss: 0.0100, Evaluation Loss 0.0110

============= Epoch 13 | 2022-09-09 10:39:44 ==============
=> Current Lr: 0.00025
[0/125]: 0.0116
[20/125]: 0.0104
[40/125]: 0.0075
[60/125]: 0.0099
[80/125]: 0.0073
[100/125]: 0.0084
[120/125]: 0.0079
=> Training Loss: 0.0090, Evaluation Loss 0.0092

============= Epoch 14 | 2022-09-09 10:40:44 ==============
=> Current Lr: 0.00025
[0/125]: 0.0073
[20/125]: 0.0086
[40/125]: 0.0115
[60/125]: 0.0093
[80/125]: 0.0089
[100/125]: 0.0087
[120/125]: 0.0096
=> Training Loss: 0.0088, Evaluation Loss 0.0094

============= Epoch 15 | 2022-09-09 10:41:47 ==============
=> Current Lr: 0.000125
[0/125]: 0.0085
[20/125]: 0.0103
[40/125]: 0.0075
[60/125]: 0.0088
[80/125]: 0.0099
[100/125]: 0.0096
[120/125]: 0.0076
=> Training Loss: 0.0083, Evaluation Loss 0.0078

============= Epoch 16 | 2022-09-09 10:42:59 ==============
=> Current Lr: 0.000125
[0/125]: 0.0063
[20/125]: 0.0082
[40/125]: 0.0088
[60/125]: 0.0079
[80/125]: 0.0066
[100/125]: 0.0073
[120/125]: 0.0070
=> Training Loss: 0.0077, Evaluation Loss 0.0082

============= Epoch 17 | 2022-09-09 10:44:08 ==============
=> Current Lr: 0.000125
[0/125]: 0.0071
[20/125]: 0.0083
[40/125]: 0.0069
[60/125]: 0.0090
[80/125]: 0.0066
[100/125]: 0.0076
[120/125]: 0.0072
=> Training Loss: 0.0077, Evaluation Loss 0.0083

============= Epoch 18 | 2022-09-09 10:45:13 ==============
=> Current Lr: 0.000125
[0/125]: 0.0071
[20/125]: 0.0075
[40/125]: 0.0061
[60/125]: 0.0067
[80/125]: 0.0081
[100/125]: 0.0079
[120/125]: 0.0073
=> Training Loss: 0.0074, Evaluation Loss 0.0079

============= Epoch 19 | 2022-09-09 10:46:19 ==============
=> Current Lr: 0.000125
[0/125]: 0.0073
[20/125]: 0.0085
[40/125]: 0.0058
[60/125]: 0.0061
[80/125]: 0.0067
[100/125]: 0.0081
[120/125]: 0.0074
=> Training Loss: 0.0073, Evaluation Loss 0.0084

============= Epoch 20 | 2022-09-09 10:47:26 ==============
=> Current Lr: 6.25e-05
[0/125]: 0.0069
[20/125]: 0.0075
[40/125]: 0.0090
[60/125]: 0.0064
[80/125]: 0.0067
[100/125]: 0.0056
[120/125]: 0.0062
=> Training Loss: 0.0069, Evaluation Loss 0.0069

============= Epoch 21 | 2022-09-09 10:48:30 ==============
=> Current Lr: 6.25e-05
[0/125]: 0.0074
[20/125]: 0.0066
[40/125]: 0.0056
[60/125]: 0.0060
[80/125]: 0.0068
[100/125]: 0.0076
[120/125]: 0.0089
=> Training Loss: 0.0068, Evaluation Loss 0.0074

============= Epoch 22 | 2022-09-09 10:49:34 ==============
=> Current Lr: 6.25e-05
[0/125]: 0.0072
[20/125]: 0.0068
[40/125]: 0.0118
[60/125]: 0.0071
[80/125]: 0.0060
[100/125]: 0.0075
[120/125]: 0.0061
=> Training Loss: 0.0075, Evaluation Loss 0.0083

============= Epoch 23 | 2022-09-09 10:50:39 ==============
=> Current Lr: 6.25e-05
[0/125]: 0.0073
[20/125]: 0.0059
[40/125]: 0.0046
[60/125]: 0.0063
[80/125]: 0.0058
[100/125]: 0.0045
[120/125]: 0.0064
=> Training Loss: 0.0066, Evaluation Loss 0.0070

============= Epoch 24 | 2022-09-09 10:51:47 ==============
=> Current Lr: 6.25e-05
[0/125]: 0.0066
[20/125]: 0.0066
[40/125]: 0.0073
[60/125]: 0.0058
[80/125]: 0.0057
[100/125]: 0.0062
[120/125]: 0.0059
=> Training Loss: 0.0065, Evaluation Loss 0.0067

============= Epoch 25 | 2022-09-09 10:52:51 ==============
=> Current Lr: 3.125e-05
[0/125]: 0.0061
[20/125]: 0.0062
[40/125]: 0.0056
[60/125]: 0.0064
[80/125]: 0.0060
[100/125]: 0.0071
[120/125]: 0.0055
=> Training Loss: 0.0064, Evaluation Loss 0.0064

============= Epoch 26 | 2022-09-09 10:53:58 ==============
=> Current Lr: 3.125e-05
[0/125]: 0.0067
[20/125]: 0.0064
[40/125]: 0.0061
[60/125]: 0.0065
[80/125]: 0.0060
[100/125]: 0.0065
[120/125]: 0.0061
=> Training Loss: 0.0063, Evaluation Loss 0.0067

============= Epoch 27 | 2022-09-09 10:55:04 ==============
=> Current Lr: 3.125e-05
[0/125]: 0.0063
[20/125]: 0.0071
[40/125]: 0.0062
[60/125]: 0.0060
[80/125]: 0.0063
[100/125]: 0.0050
[120/125]: 0.0065
=> Training Loss: 0.0063, Evaluation Loss 0.0065

============= Epoch 28 | 2022-09-09 10:56:10 ==============
=> Current Lr: 3.125e-05
[0/125]: 0.0060
[20/125]: 0.0057
[40/125]: 0.0061
[60/125]: 0.0064
[80/125]: 0.0066
[100/125]: 0.0058
[120/125]: 0.0057
=> Training Loss: 0.0062, Evaluation Loss 0.0067

============= Epoch 29 | 2022-09-09 10:57:15 ==============
=> Current Lr: 3.125e-05
[0/125]: 0.0062
[20/125]: 0.0072
[40/125]: 0.0053
[60/125]: 0.0049
[80/125]: 0.0057
[100/125]: 0.0061
[120/125]: 0.0080
=> Training Loss: 0.0063, Evaluation Loss 0.0065

============= Epoch 30 | 2022-09-09 10:58:21 ==============
=> Current Lr: 1.5625e-05
[0/125]: 0.0065
[20/125]: 0.0063
[40/125]: 0.0060
[60/125]: 0.0061
[80/125]: 0.0056
[100/125]: 0.0060
[120/125]: 0.0068
=> Training Loss: 0.0061, Evaluation Loss 0.0063

============= Epoch 31 | 2022-09-09 10:59:27 ==============
=> Current Lr: 1.5625e-05
[0/125]: 0.0046
[20/125]: 0.0048
[40/125]: 0.0061
[60/125]: 0.0058
[80/125]: 0.0068
[100/125]: 0.0066
[120/125]: 0.0066
=> Training Loss: 0.0060, Evaluation Loss 0.0062

============= Epoch 32 | 2022-09-09 11:00:31 ==============
=> Current Lr: 1.5625e-05
[0/125]: 0.0063
[20/125]: 0.0066
[40/125]: 0.0055
[60/125]: 0.0056
[80/125]: 0.0060
[100/125]: 0.0048
[120/125]: 0.0053
=> Training Loss: 0.0060, Evaluation Loss 0.0062

============= Epoch 33 | 2022-09-09 11:01:34 ==============
=> Current Lr: 1.5625e-05
[0/125]: 0.0064
[20/125]: 0.0060
[40/125]: 0.0057
[60/125]: 0.0060
[80/125]: 0.0076
[100/125]: 0.0058
[120/125]: 0.0066
=> Training Loss: 0.0060, Evaluation Loss 0.0063

============= Epoch 34 | 2022-09-09 11:02:35 ==============
=> Current Lr: 1.5625e-05
[0/125]: 0.0056
[20/125]: 0.0052
[40/125]: 0.0054
[60/125]: 0.0050
[80/125]: 0.0062
[100/125]: 0.0061
[120/125]: 0.0068
=> Training Loss: 0.0060, Evaluation Loss 0.0063

============= Epoch 35 | 2022-09-09 11:03:35 ==============
=> Current Lr: 7.8125e-06
[0/125]: 0.0050
[20/125]: 0.0050
[40/125]: 0.0053
[60/125]: 0.0054
[80/125]: 0.0069
[100/125]: 0.0072
[120/125]: 0.0070
=> Training Loss: 0.0060, Evaluation Loss 0.0061

============= Epoch 36 | 2022-09-09 11:04:40 ==============
=> Current Lr: 7.8125e-06
[0/125]: 0.0058
[20/125]: 0.0062
[40/125]: 0.0053
[60/125]: 0.0053
[80/125]: 0.0057
[100/125]: 0.0054
[120/125]: 0.0054
=> Training Loss: 0.0059, Evaluation Loss 0.0062

============= Epoch 37 | 2022-09-09 11:05:47 ==============
=> Current Lr: 7.8125e-06
[0/125]: 0.0059
[20/125]: 0.0056
[40/125]: 0.0049
[60/125]: 0.0056
[80/125]: 0.0046
[100/125]: 0.0051
[120/125]: 0.0051
=> Training Loss: 0.0058, Evaluation Loss 0.0061

============= Epoch 38 | 2022-09-09 11:06:54 ==============
=> Current Lr: 7.8125e-06
[0/125]: 0.0059
[20/125]: 0.0059
[40/125]: 0.0063
[60/125]: 0.0047
[80/125]: 0.0049
[100/125]: 0.0071
[120/125]: 0.0064
=> Training Loss: 0.0059, Evaluation Loss 0.0062

============= Epoch 39 | 2022-09-09 11:08:03 ==============
=> Current Lr: 7.8125e-06
[0/125]: 0.0052
[20/125]: 0.0061
[40/125]: 0.0052
[60/125]: 0.0048
[80/125]: 0.0054
[100/125]: 0.0057
[120/125]: 0.0056
=> Training Loss: 0.0058, Evaluation Loss 0.0061
